{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install efficientnet"
      ],
      "metadata": {
        "id": "Sia4xYoLHwyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50, DenseNet121,EfficientNetB0\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Define batch size and number of epochs\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# Define the CNN model\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the CNN model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "cnn_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Define the ResNet model\n",
        "resnet_model = ResNet50(weights=None, input_shape=(32, 32, 3), classes=10)\n",
        "\n",
        "# Compile and train the ResNet model\n",
        "resnet_model.compile(optimizer='adam',\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "resnet_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Define the DenseNet model\n",
        "densenet_model = DenseNet121(weights=None, input_shape=(32, 32, 3), classes=10)\n",
        "\n",
        "# Compile and train the DenseNet model\n",
        "densenet_model.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "densenet_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Define the EfficientNetB0 model\n",
        "efficientnet_model = EfficientNetB0(weights=None, input_shape=(32, 32, 3), classes=10)\n",
        "\n",
        "# Compile and train the EfficientNetB0 model\n",
        "efficientnet_model.compile(optimizer='adam',\n",
        "                          loss='sparse_categorical_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "efficientnet_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "# Evaluate models and generate classification reports\n",
        "def evaluate_model(model, name):\n",
        "    print(f\"Evaluating {name} Model:\")\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "\n",
        "evaluate_model(cnn_model, \"CNN\")\n",
        "evaluate_model(resnet_model, \"ResNet\")\n",
        "evaluate_model(densenet_model, \"DenseNet\")\n",
        "evaluate_model(efficientnet_model, \"EfficientNet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtBthrcXB6LM",
        "outputId": "6b48d14d-422f-47a3-c317-56e457b2ed98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 8s 6ms/step - loss: 1.5434 - accuracy: 0.4350 - val_loss: 1.3603 - val_accuracy: 0.5250\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.1836 - accuracy: 0.5818 - val_loss: 1.1791 - val_accuracy: 0.5836\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.0243 - accuracy: 0.6410 - val_loss: 0.9790 - val_accuracy: 0.6578\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.9136 - accuracy: 0.6830 - val_loss: 0.9958 - val_accuracy: 0.6529\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.8303 - accuracy: 0.7124 - val_loss: 0.8990 - val_accuracy: 0.6889\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.7630 - accuracy: 0.7345 - val_loss: 0.9072 - val_accuracy: 0.6920\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.6998 - accuracy: 0.7577 - val_loss: 0.8227 - val_accuracy: 0.7146\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.6530 - accuracy: 0.7750 - val_loss: 0.8319 - val_accuracy: 0.7143\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5943 - accuracy: 0.7941 - val_loss: 0.8371 - val_accuracy: 0.7214\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.5552 - accuracy: 0.8071 - val_loss: 0.8476 - val_accuracy: 0.7106\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 75s 50ms/step - loss: 2.0151 - accuracy: 0.3489 - val_loss: 2.9788 - val_accuracy: 0.1658\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 1.8293 - accuracy: 0.4064 - val_loss: 7.1139 - val_accuracy: 0.3227\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 1.6738 - accuracy: 0.4471 - val_loss: 1.7570 - val_accuracy: 0.4301\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 1.8741 - accuracy: 0.3842 - val_loss: 1.6795 - val_accuracy: 0.3892\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 1.6988 - accuracy: 0.4248 - val_loss: 2.4784 - val_accuracy: 0.2655\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 1.5143 - accuracy: 0.4907 - val_loss: 3.3058 - val_accuracy: 0.4678\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 1.3920 - accuracy: 0.5378 - val_loss: 1.6073 - val_accuracy: 0.4666\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 1.3542 - accuracy: 0.5520 - val_loss: 1.7041 - val_accuracy: 0.4248\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 38s 49ms/step - loss: 1.2825 - accuracy: 0.5842 - val_loss: 1.8617 - val_accuracy: 0.3557\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 1.1952 - accuracy: 0.6054 - val_loss: 1.6321 - val_accuracy: 0.4528\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 128s 83ms/step - loss: 1.4603 - accuracy: 0.4761 - val_loss: 2.0163 - val_accuracy: 0.3482\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 62s 80ms/step - loss: 1.0683 - accuracy: 0.6241 - val_loss: 1.6133 - val_accuracy: 0.4793\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 63s 80ms/step - loss: 0.8959 - accuracy: 0.6891 - val_loss: 1.6908 - val_accuracy: 0.4990\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 63s 81ms/step - loss: 0.7646 - accuracy: 0.7337 - val_loss: 0.8462 - val_accuracy: 0.7092\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 61s 78ms/step - loss: 0.6900 - accuracy: 0.7626 - val_loss: 1.0057 - val_accuracy: 0.6625\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 62s 80ms/step - loss: 0.5690 - accuracy: 0.8027 - val_loss: 1.4485 - val_accuracy: 0.5678\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 62s 79ms/step - loss: 0.5149 - accuracy: 0.8203 - val_loss: 0.7560 - val_accuracy: 0.7484\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 63s 80ms/step - loss: 0.4173 - accuracy: 0.8546 - val_loss: 0.9074 - val_accuracy: 0.7066\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 61s 78ms/step - loss: 0.4148 - accuracy: 0.8565 - val_loss: 2.2493 - val_accuracy: 0.5152\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 62s 79ms/step - loss: 0.3002 - accuracy: 0.8937 - val_loss: 0.9238 - val_accuracy: 0.7262\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 85s 53ms/step - loss: 2.6014 - accuracy: 0.2221 - val_loss: 2.0499 - val_accuracy: 0.2789\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 38s 48ms/step - loss: 2.1634 - accuracy: 0.3043 - val_loss: 1.9577 - val_accuracy: 0.2906\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 2.1135 - accuracy: 0.3214 - val_loss: 2.7578 - val_accuracy: 0.2771\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 1.9494 - accuracy: 0.3736 - val_loss: 2.4627 - val_accuracy: 0.3866\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 2.0180 - accuracy: 0.3546 - val_loss: 1.8823 - val_accuracy: 0.3064\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 38s 49ms/step - loss: 1.9224 - accuracy: 0.3759 - val_loss: 1.7956 - val_accuracy: 0.3739\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 38s 48ms/step - loss: 1.8158 - accuracy: 0.4012 - val_loss: 1.6198 - val_accuracy: 0.4183\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 38s 49ms/step - loss: 1.7752 - accuracy: 0.4176 - val_loss: 2.0069 - val_accuracy: 0.2611\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 38s 49ms/step - loss: 1.9291 - accuracy: 0.3800 - val_loss: 1.7890 - val_accuracy: 0.4224\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 38s 48ms/step - loss: 1.8846 - accuracy: 0.3930 - val_loss: 1.8067 - val_accuracy: 0.3921\n",
            "Evaluating CNN Model:\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.84      0.74      1000\n",
            "           1       0.80      0.84      0.82      1000\n",
            "           2       0.62      0.63      0.63      1000\n",
            "           3       0.55      0.52      0.53      1000\n",
            "           4       0.75      0.57      0.65      1000\n",
            "           5       0.56      0.72      0.63      1000\n",
            "           6       0.82      0.75      0.79      1000\n",
            "           7       0.74      0.76      0.75      1000\n",
            "           8       0.82      0.82      0.82      1000\n",
            "           9       0.90      0.66      0.76      1000\n",
            "\n",
            "    accuracy                           0.71     10000\n",
            "   macro avg       0.72      0.71      0.71     10000\n",
            "weighted avg       0.72      0.71      0.71     10000\n",
            "\n",
            "Evaluating ResNet Model:\n",
            "313/313 [==============================] - 4s 8ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.55      0.55      1000\n",
            "           1       0.60      0.64      0.62      1000\n",
            "           2       0.32      0.64      0.42      1000\n",
            "           3       0.34      0.39      0.36      1000\n",
            "           4       0.59      0.15      0.24      1000\n",
            "           5       0.45      0.39      0.42      1000\n",
            "           6       0.35      0.86      0.49      1000\n",
            "           7       0.84      0.31      0.46      1000\n",
            "           8       0.74      0.55      0.63      1000\n",
            "           9       0.98      0.04      0.08      1000\n",
            "\n",
            "    accuracy                           0.45     10000\n",
            "   macro avg       0.58      0.45      0.43     10000\n",
            "weighted avg       0.58      0.45      0.43     10000\n",
            "\n",
            "Evaluating DenseNet Model:\n",
            "313/313 [==============================] - 7s 13ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.70      0.76      1000\n",
            "           1       0.86      0.84      0.85      1000\n",
            "           2       0.74      0.61      0.67      1000\n",
            "           3       0.45      0.71      0.55      1000\n",
            "           4       0.82      0.57      0.67      1000\n",
            "           5       0.63      0.62      0.62      1000\n",
            "           6       0.70      0.85      0.77      1000\n",
            "           7       0.91      0.62      0.74      1000\n",
            "           8       0.85      0.85      0.85      1000\n",
            "           9       0.73      0.88      0.80      1000\n",
            "\n",
            "    accuracy                           0.73     10000\n",
            "   macro avg       0.75      0.73      0.73     10000\n",
            "weighted avg       0.75      0.73      0.73     10000\n",
            "\n",
            "Evaluating EfficientNet Model:\n",
            "313/313 [==============================] - 5s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.32      0.42      1000\n",
            "           1       0.58      0.39      0.47      1000\n",
            "           2       0.34      0.17      0.22      1000\n",
            "           3       0.27      0.22      0.24      1000\n",
            "           4       0.27      0.42      0.32      1000\n",
            "           5       0.35      0.36      0.36      1000\n",
            "           6       0.37      0.54      0.44      1000\n",
            "           7       0.37      0.48      0.42      1000\n",
            "           8       0.58      0.42      0.49      1000\n",
            "           9       0.43      0.61      0.50      1000\n",
            "\n",
            "    accuracy                           0.39     10000\n",
            "   macro avg       0.42      0.39      0.39     10000\n",
            "weighted avg       0.42      0.39      0.39     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.applications import ResNet50, DenseNet121, EfficientNetB0\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Define batch size and number of epochs\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# Define the CNN model\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the CNN model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "cnn_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Define the ResNet model\n",
        "resnet_model = ResNet50(weights=None, input_shape=(32, 32, 3), classes=100)\n",
        "\n",
        "# Compile and train the ResNet model\n",
        "resnet_model.compile(optimizer='adam',\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "resnet_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Define the DenseNet model\n",
        "densenet_model = DenseNet121(weights=None, input_shape=(32, 32, 3), classes=100)\n",
        "\n",
        "# Compile and train the DenseNet model\n",
        "densenet_model.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "densenet_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Define the EfficientNetB0 model\n",
        "efficientnet_model = EfficientNetB0(weights=None, input_shape=(32, 32, 3), classes=100)\n",
        "\n",
        "# Compile and train the EfficientNetB0 model\n",
        "efficientnet_model.compile(optimizer='adam',\n",
        "                          loss='sparse_categorical_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "efficientnet_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate models and generate classification reports\n",
        "def evaluate_model(model, name):\n",
        "    print(f\"Evaluating {name} Model:\")\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "\n",
        "evaluate_model(cnn_model, \"CNN\")\n",
        "evaluate_model(resnet_model, \"ResNet\")\n",
        "evaluate_model(densenet_model, \"DenseNet\")\n",
        "evaluate_model(efficientnet_model, \"EfficientNet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFqLy7uf_yiE",
        "outputId": "e6e67649-3b65-462d-fcea-4326a255d12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 14s 0us/step\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 7s 6ms/step - loss: 3.9645 - accuracy: 0.0943 - val_loss: 3.5393 - val_accuracy: 0.1643\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 3.2900 - accuracy: 0.2067 - val_loss: 3.1217 - val_accuracy: 0.2403\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 2.9660 - accuracy: 0.2687 - val_loss: 2.8895 - val_accuracy: 0.2892\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.7680 - accuracy: 0.3055 - val_loss: 2.7709 - val_accuracy: 0.3141\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.5986 - accuracy: 0.3414 - val_loss: 2.7163 - val_accuracy: 0.3237\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 2.4726 - accuracy: 0.3680 - val_loss: 2.6675 - val_accuracy: 0.3315\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3630 - accuracy: 0.3904 - val_loss: 2.5722 - val_accuracy: 0.3482\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.2644 - accuracy: 0.4125 - val_loss: 2.5213 - val_accuracy: 0.3636\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 2.1801 - accuracy: 0.4297 - val_loss: 2.5132 - val_accuracy: 0.3629\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.0991 - accuracy: 0.4453 - val_loss: 2.5136 - val_accuracy: 0.3718\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 76s 52ms/step - loss: 4.7827 - accuracy: 0.0670 - val_loss: 19.4095 - val_accuracy: 0.0428\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 4.1637 - accuracy: 0.1253 - val_loss: 4.6098 - val_accuracy: 0.0627\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 4.6347 - accuracy: 0.0585 - val_loss: 5.1356 - val_accuracy: 0.0674\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 4.0266 - accuracy: 0.1054 - val_loss: 6.3785 - val_accuracy: 0.1184\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 3.8331 - accuracy: 0.1367 - val_loss: 5.1704 - val_accuracy: 0.1314\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 38s 49ms/step - loss: 3.6643 - accuracy: 0.1686 - val_loss: 3.7296 - val_accuracy: 0.1476\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 39s 49ms/step - loss: 3.6592 - accuracy: 0.1592 - val_loss: 4.2258 - val_accuracy: 0.1692\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 3.5278 - accuracy: 0.1797 - val_loss: 4.3106 - val_accuracy: 0.1931\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 36s 47ms/step - loss: 3.2368 - accuracy: 0.2231 - val_loss: 3.7705 - val_accuracy: 0.1394\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 38s 48ms/step - loss: 3.0643 - accuracy: 0.2531 - val_loss: 3.1201 - val_accuracy: 0.2438\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 134s 85ms/step - loss: 3.7091 - accuracy: 0.1409 - val_loss: 5.1485 - val_accuracy: 0.0962\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 2.9164 - accuracy: 0.2727 - val_loss: 4.1705 - val_accuracy: 0.1710\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 2.5031 - accuracy: 0.3521 - val_loss: 3.1912 - val_accuracy: 0.2419\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 63s 81ms/step - loss: 2.1790 - accuracy: 0.4191 - val_loss: 2.6918 - val_accuracy: 0.3271\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 63s 81ms/step - loss: 1.9204 - accuracy: 0.4777 - val_loss: 2.3845 - val_accuracy: 0.3867\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 1.7023 - accuracy: 0.5278 - val_loss: 2.7041 - val_accuracy: 0.3597\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 1.4968 - accuracy: 0.5762 - val_loss: 2.5813 - val_accuracy: 0.4127\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 64s 81ms/step - loss: 1.2811 - accuracy: 0.6279 - val_loss: 2.4201 - val_accuracy: 0.4124\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 62s 79ms/step - loss: 1.0841 - accuracy: 0.6782 - val_loss: 2.4444 - val_accuracy: 0.4332\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.8751 - accuracy: 0.7333 - val_loss: 2.8738 - val_accuracy: 0.3890\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 82s 53ms/step - loss: 4.6148 - accuracy: 0.0407 - val_loss: 4.2760 - val_accuracy: 0.0549\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 4.1222 - accuracy: 0.0824 - val_loss: 4.0143 - val_accuracy: 0.1023\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 3.8908 - accuracy: 0.1084 - val_loss: 3.7346 - val_accuracy: 0.1321\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 3.6885 - accuracy: 0.1377 - val_loss: 3.6337 - val_accuracy: 0.1606\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 3.4977 - accuracy: 0.1667 - val_loss: 3.5013 - val_accuracy: 0.1661\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 3.3079 - accuracy: 0.1985 - val_loss: 3.2597 - val_accuracy: 0.2128\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 3.1379 - accuracy: 0.2280 - val_loss: 3.1443 - val_accuracy: 0.2293\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 38s 49ms/step - loss: 2.9786 - accuracy: 0.2559 - val_loss: 3.0652 - val_accuracy: 0.2537\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 39s 49ms/step - loss: 2.8427 - accuracy: 0.2841 - val_loss: 3.2117 - val_accuracy: 0.2271\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 2.7163 - accuracy: 0.3028 - val_loss: 2.7921 - val_accuracy: 0.2972\n",
            "Evaluating CNN Model:\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.71      0.69       100\n",
            "           1       0.44      0.38      0.41       100\n",
            "           2       0.20      0.21      0.20       100\n",
            "           3       0.23      0.17      0.19       100\n",
            "           4       0.27      0.20      0.23       100\n",
            "           5       0.31      0.34      0.32       100\n",
            "           6       0.49      0.46      0.48       100\n",
            "           7       0.58      0.30      0.39       100\n",
            "           8       0.53      0.33      0.41       100\n",
            "           9       0.39      0.50      0.44       100\n",
            "          10       0.23      0.22      0.22       100\n",
            "          11       0.25      0.15      0.19       100\n",
            "          12       0.42      0.30      0.35       100\n",
            "          13       0.21      0.43      0.28       100\n",
            "          14       0.34      0.33      0.33       100\n",
            "          15       0.34      0.17      0.23       100\n",
            "          16       0.40      0.41      0.40       100\n",
            "          17       0.52      0.48      0.50       100\n",
            "          18       0.37      0.25      0.30       100\n",
            "          19       0.32      0.18      0.23       100\n",
            "          20       0.69      0.61      0.65       100\n",
            "          21       0.35      0.73      0.47       100\n",
            "          22       0.45      0.25      0.32       100\n",
            "          23       0.52      0.68      0.59       100\n",
            "          24       0.56      0.65      0.60       100\n",
            "          25       0.42      0.19      0.26       100\n",
            "          26       0.46      0.24      0.32       100\n",
            "          27       0.24      0.29      0.26       100\n",
            "          28       0.50      0.52      0.51       100\n",
            "          29       0.50      0.26      0.34       100\n",
            "          30       0.34      0.43      0.38       100\n",
            "          31       0.32      0.34      0.33       100\n",
            "          32       0.44      0.27      0.33       100\n",
            "          33       0.38      0.47      0.42       100\n",
            "          34       0.27      0.38      0.32       100\n",
            "          35       0.19      0.20      0.19       100\n",
            "          36       0.38      0.33      0.35       100\n",
            "          37       0.21      0.32      0.25       100\n",
            "          38       0.16      0.21      0.18       100\n",
            "          39       0.49      0.33      0.40       100\n",
            "          40       0.35      0.30      0.32       100\n",
            "          41       0.74      0.52      0.61       100\n",
            "          42       0.45      0.26      0.33       100\n",
            "          43       0.47      0.43      0.45       100\n",
            "          44       0.41      0.09      0.15       100\n",
            "          45       0.20      0.17      0.18       100\n",
            "          46       0.26      0.30      0.28       100\n",
            "          47       0.49      0.50      0.49       100\n",
            "          48       0.41      0.68      0.51       100\n",
            "          49       0.37      0.70      0.49       100\n",
            "          50       0.59      0.10      0.17       100\n",
            "          51       0.36      0.27      0.31       100\n",
            "          52       0.56      0.54      0.55       100\n",
            "          53       0.56      0.44      0.49       100\n",
            "          54       0.62      0.36      0.46       100\n",
            "          55       0.17      0.06      0.09       100\n",
            "          56       0.36      0.62      0.46       100\n",
            "          57       0.46      0.30      0.36       100\n",
            "          58       0.33      0.45      0.38       100\n",
            "          59       0.32      0.38      0.35       100\n",
            "          60       0.80      0.70      0.75       100\n",
            "          61       0.53      0.45      0.49       100\n",
            "          62       0.54      0.37      0.44       100\n",
            "          63       0.31      0.38      0.34       100\n",
            "          64       0.14      0.21      0.17       100\n",
            "          65       0.28      0.16      0.20       100\n",
            "          66       0.41      0.26      0.32       100\n",
            "          67       0.30      0.42      0.35       100\n",
            "          68       0.81      0.54      0.65       100\n",
            "          69       0.37      0.58      0.45       100\n",
            "          70       0.39      0.55      0.45       100\n",
            "          71       0.54      0.67      0.60       100\n",
            "          72       0.14      0.34      0.19       100\n",
            "          73       0.36      0.41      0.38       100\n",
            "          74       0.29      0.12      0.17       100\n",
            "          75       0.64      0.61      0.63       100\n",
            "          76       0.45      0.73      0.56       100\n",
            "          77       0.34      0.13      0.19       100\n",
            "          78       0.19      0.04      0.07       100\n",
            "          79       0.36      0.36      0.36       100\n",
            "          80       0.18      0.19      0.19       100\n",
            "          81       0.25      0.61      0.36       100\n",
            "          82       0.71      0.63      0.67       100\n",
            "          83       0.33      0.26      0.29       100\n",
            "          84       0.29      0.15      0.20       100\n",
            "          85       0.56      0.36      0.44       100\n",
            "          86       0.48      0.36      0.41       100\n",
            "          87       0.34      0.44      0.39       100\n",
            "          88       0.38      0.33      0.35       100\n",
            "          89       0.23      0.46      0.31       100\n",
            "          90       0.29      0.21      0.24       100\n",
            "          91       0.48      0.48      0.48       100\n",
            "          92       0.37      0.10      0.16       100\n",
            "          93       0.20      0.22      0.21       100\n",
            "          94       0.55      0.76      0.64       100\n",
            "          95       0.36      0.57      0.44       100\n",
            "          96       0.31      0.20      0.24       100\n",
            "          97       0.28      0.45      0.34       100\n",
            "          98       0.15      0.24      0.18       100\n",
            "          99       0.44      0.44      0.44       100\n",
            "\n",
            "    accuracy                           0.37     10000\n",
            "   macro avg       0.39      0.37      0.36     10000\n",
            "weighted avg       0.39      0.37      0.36     10000\n",
            "\n",
            "Evaluating ResNet Model:\n",
            "313/313 [==============================] - 3s 8ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.53      0.54       100\n",
            "           1       0.24      0.31      0.27       100\n",
            "           2       0.21      0.25      0.23       100\n",
            "           3       0.14      0.04      0.06       100\n",
            "           4       0.09      0.08      0.08       100\n",
            "           5       0.24      0.15      0.19       100\n",
            "           6       0.46      0.25      0.32       100\n",
            "           7       0.25      0.28      0.27       100\n",
            "           8       0.22      0.21      0.21       100\n",
            "           9       0.46      0.23      0.31       100\n",
            "          10       0.05      0.02      0.03       100\n",
            "          11       0.24      0.10      0.14       100\n",
            "          12       0.29      0.25      0.27       100\n",
            "          13       0.23      0.12      0.16       100\n",
            "          14       0.16      0.32      0.22       100\n",
            "          15       0.14      0.08      0.10       100\n",
            "          16       0.22      0.25      0.23       100\n",
            "          17       0.41      0.21      0.28       100\n",
            "          18       0.23      0.23      0.23       100\n",
            "          19       0.25      0.15      0.19       100\n",
            "          20       0.35      0.41      0.38       100\n",
            "          21       0.26      0.54      0.36       100\n",
            "          22       0.26      0.09      0.13       100\n",
            "          23       0.32      0.59      0.41       100\n",
            "          24       0.26      0.48      0.33       100\n",
            "          25       0.23      0.10      0.14       100\n",
            "          26       0.24      0.12      0.16       100\n",
            "          27       0.13      0.19      0.15       100\n",
            "          28       0.21      0.39      0.27       100\n",
            "          29       0.25      0.06      0.10       100\n",
            "          30       0.20      0.48      0.28       100\n",
            "          31       0.16      0.08      0.11       100\n",
            "          32       0.19      0.17      0.18       100\n",
            "          33       0.17      0.46      0.25       100\n",
            "          34       0.19      0.12      0.15       100\n",
            "          35       0.08      0.03      0.04       100\n",
            "          36       0.29      0.02      0.04       100\n",
            "          37       0.30      0.14      0.19       100\n",
            "          38       0.00      0.00      0.00       100\n",
            "          39       0.15      0.28      0.20       100\n",
            "          40       0.17      0.07      0.10       100\n",
            "          41       0.45      0.49      0.47       100\n",
            "          42       0.17      0.26      0.20       100\n",
            "          43       0.31      0.04      0.07       100\n",
            "          44       0.11      0.04      0.06       100\n",
            "          45       0.10      0.05      0.07       100\n",
            "          46       0.36      0.10      0.16       100\n",
            "          47       0.32      0.49      0.38       100\n",
            "          48       0.17      0.53      0.26       100\n",
            "          49       0.31      0.45      0.37       100\n",
            "          50       0.07      0.01      0.02       100\n",
            "          51       0.11      0.08      0.09       100\n",
            "          52       0.37      0.53      0.43       100\n",
            "          53       0.57      0.43      0.49       100\n",
            "          54       0.32      0.23      0.27       100\n",
            "          55       0.06      0.04      0.05       100\n",
            "          56       0.26      0.38      0.31       100\n",
            "          57       0.55      0.17      0.26       100\n",
            "          58       0.18      0.46      0.26       100\n",
            "          59       0.11      0.07      0.09       100\n",
            "          60       0.58      0.66      0.62       100\n",
            "          61       0.23      0.35      0.28       100\n",
            "          62       0.38      0.38      0.38       100\n",
            "          63       0.34      0.21      0.26       100\n",
            "          64       0.06      0.14      0.08       100\n",
            "          65       0.41      0.07      0.12       100\n",
            "          66       0.09      0.05      0.06       100\n",
            "          67       0.24      0.33      0.28       100\n",
            "          68       0.61      0.46      0.52       100\n",
            "          69       0.60      0.21      0.31       100\n",
            "          70       0.32      0.24      0.27       100\n",
            "          71       0.32      0.57      0.41       100\n",
            "          72       0.10      0.05      0.07       100\n",
            "          73       0.23      0.25      0.24       100\n",
            "          74       0.20      0.12      0.15       100\n",
            "          75       0.26      0.68      0.37       100\n",
            "          76       0.38      0.57      0.45       100\n",
            "          77       0.22      0.02      0.04       100\n",
            "          78       0.09      0.02      0.03       100\n",
            "          79       0.18      0.22      0.20       100\n",
            "          80       0.12      0.09      0.10       100\n",
            "          81       0.14      0.15      0.15       100\n",
            "          82       0.59      0.63      0.61       100\n",
            "          83       0.27      0.24      0.26       100\n",
            "          84       0.14      0.05      0.07       100\n",
            "          85       0.38      0.29      0.33       100\n",
            "          86       0.15      0.36      0.21       100\n",
            "          87       0.17      0.29      0.21       100\n",
            "          88       0.23      0.10      0.14       100\n",
            "          89       0.33      0.18      0.23       100\n",
            "          90       0.24      0.24      0.24       100\n",
            "          91       0.23      0.36      0.28       100\n",
            "          92       0.21      0.27      0.24       100\n",
            "          93       0.18      0.12      0.14       100\n",
            "          94       0.45      0.48      0.46       100\n",
            "          95       0.24      0.42      0.31       100\n",
            "          96       0.37      0.20      0.26       100\n",
            "          97       0.14      0.28      0.18       100\n",
            "          98       0.10      0.05      0.07       100\n",
            "          99       0.18      0.25      0.21       100\n",
            "\n",
            "    accuracy                           0.24     10000\n",
            "   macro avg       0.25      0.24      0.22     10000\n",
            "weighted avg       0.25      0.24      0.22     10000\n",
            "\n",
            "Evaluating DenseNet Model:\n",
            "313/313 [==============================] - 6s 12ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.45      0.59       100\n",
            "           1       0.64      0.49      0.56       100\n",
            "           2       0.39      0.18      0.25       100\n",
            "           3       0.51      0.18      0.27       100\n",
            "           4       0.26      0.11      0.15       100\n",
            "           5       0.65      0.15      0.24       100\n",
            "           6       0.67      0.28      0.39       100\n",
            "           7       0.39      0.58      0.46       100\n",
            "           8       0.39      0.54      0.45       100\n",
            "           9       0.50      0.45      0.47       100\n",
            "          10       0.33      0.22      0.27       100\n",
            "          11       0.44      0.16      0.24       100\n",
            "          12       0.64      0.27      0.38       100\n",
            "          13       0.61      0.33      0.43       100\n",
            "          14       0.28      0.41      0.33       100\n",
            "          15       0.31      0.39      0.34       100\n",
            "          16       1.00      0.14      0.25       100\n",
            "          17       0.83      0.05      0.09       100\n",
            "          18       0.59      0.20      0.30       100\n",
            "          19       0.60      0.21      0.31       100\n",
            "          20       0.88      0.44      0.59       100\n",
            "          21       0.69      0.58      0.63       100\n",
            "          22       0.18      0.49      0.26       100\n",
            "          23       0.56      0.43      0.49       100\n",
            "          24       0.73      0.60      0.66       100\n",
            "          25       0.61      0.19      0.29       100\n",
            "          26       0.16      0.53      0.24       100\n",
            "          27       0.23      0.32      0.27       100\n",
            "          28       0.69      0.49      0.57       100\n",
            "          29       0.37      0.41      0.39       100\n",
            "          30       0.39      0.37      0.38       100\n",
            "          31       0.75      0.21      0.33       100\n",
            "          32       0.35      0.44      0.39       100\n",
            "          33       0.62      0.24      0.35       100\n",
            "          34       0.54      0.40      0.46       100\n",
            "          35       0.34      0.21      0.26       100\n",
            "          36       0.25      0.47      0.33       100\n",
            "          37       0.63      0.22      0.33       100\n",
            "          38       0.19      0.32      0.24       100\n",
            "          39       0.22      0.68      0.33       100\n",
            "          40       0.28      0.34      0.30       100\n",
            "          41       0.79      0.54      0.64       100\n",
            "          42       0.31      0.50      0.38       100\n",
            "          43       0.42      0.25      0.31       100\n",
            "          44       0.19      0.21      0.20       100\n",
            "          45       0.25      0.33      0.28       100\n",
            "          46       0.38      0.36      0.37       100\n",
            "          47       0.57      0.54      0.55       100\n",
            "          48       0.46      0.75      0.57       100\n",
            "          49       0.29      0.75      0.42       100\n",
            "          50       0.26      0.16      0.20       100\n",
            "          51       0.30      0.57      0.39       100\n",
            "          52       0.53      0.59      0.56       100\n",
            "          53       0.55      0.78      0.64       100\n",
            "          54       0.75      0.33      0.46       100\n",
            "          55       0.26      0.11      0.15       100\n",
            "          56       0.64      0.73      0.68       100\n",
            "          57       0.56      0.41      0.47       100\n",
            "          58       0.66      0.42      0.51       100\n",
            "          59       0.31      0.48      0.38       100\n",
            "          60       0.85      0.63      0.72       100\n",
            "          61       0.46      0.35      0.40       100\n",
            "          62       0.48      0.57      0.52       100\n",
            "          63       0.62      0.34      0.44       100\n",
            "          64       0.22      0.26      0.24       100\n",
            "          65       0.33      0.21      0.26       100\n",
            "          66       0.57      0.21      0.31       100\n",
            "          67       0.20      0.39      0.26       100\n",
            "          68       0.84      0.70      0.77       100\n",
            "          69       0.83      0.43      0.57       100\n",
            "          70       0.52      0.28      0.36       100\n",
            "          71       0.63      0.32      0.42       100\n",
            "          72       0.20      0.10      0.13       100\n",
            "          73       0.38      0.34      0.36       100\n",
            "          74       0.21      0.41      0.28       100\n",
            "          75       0.71      0.70      0.71       100\n",
            "          76       0.89      0.40      0.55       100\n",
            "          77       0.28      0.31      0.29       100\n",
            "          78       0.12      0.54      0.20       100\n",
            "          79       0.42      0.33      0.37       100\n",
            "          80       0.32      0.10      0.15       100\n",
            "          81       0.61      0.36      0.45       100\n",
            "          82       0.77      0.81      0.79       100\n",
            "          83       0.34      0.49      0.40       100\n",
            "          84       0.67      0.10      0.17       100\n",
            "          85       0.57      0.53      0.55       100\n",
            "          86       0.48      0.36      0.41       100\n",
            "          87       0.70      0.43      0.53       100\n",
            "          88       0.36      0.51      0.42       100\n",
            "          89       0.49      0.46      0.47       100\n",
            "          90       0.68      0.27      0.39       100\n",
            "          91       0.52      0.56      0.54       100\n",
            "          92       0.41      0.30      0.34       100\n",
            "          93       0.19      0.46      0.27       100\n",
            "          94       0.96      0.46      0.62       100\n",
            "          95       0.28      0.71      0.40       100\n",
            "          96       0.43      0.06      0.11       100\n",
            "          97       0.33      0.39      0.36       100\n",
            "          98       0.28      0.27      0.28       100\n",
            "          99       0.24      0.47      0.32       100\n",
            "\n",
            "    accuracy                           0.39     10000\n",
            "   macro avg       0.48      0.39      0.39     10000\n",
            "weighted avg       0.48      0.39      0.39     10000\n",
            "\n",
            "Evaluating EfficientNet Model:\n",
            "313/313 [==============================] - 4s 8ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.58      0.54       100\n",
            "           1       0.30      0.40      0.34       100\n",
            "           2       0.38      0.06      0.10       100\n",
            "           3       0.31      0.15      0.20       100\n",
            "           4       0.15      0.17      0.16       100\n",
            "           5       0.25      0.26      0.25       100\n",
            "           6       0.36      0.28      0.31       100\n",
            "           7       0.54      0.30      0.38       100\n",
            "           8       0.29      0.34      0.31       100\n",
            "           9       0.53      0.31      0.39       100\n",
            "          10       0.18      0.19      0.19       100\n",
            "          11       0.18      0.03      0.05       100\n",
            "          12       0.23      0.40      0.29       100\n",
            "          13       0.33      0.14      0.20       100\n",
            "          14       0.15      0.20      0.17       100\n",
            "          15       0.12      0.22      0.16       100\n",
            "          16       0.41      0.29      0.34       100\n",
            "          17       0.45      0.55      0.49       100\n",
            "          18       0.21      0.39      0.27       100\n",
            "          19       0.26      0.16      0.20       100\n",
            "          20       0.74      0.46      0.57       100\n",
            "          21       0.39      0.53      0.45       100\n",
            "          22       0.21      0.05      0.08       100\n",
            "          23       0.40      0.52      0.45       100\n",
            "          24       0.59      0.47      0.52       100\n",
            "          25       0.14      0.06      0.08       100\n",
            "          26       0.22      0.17      0.19       100\n",
            "          27       0.14      0.25      0.18       100\n",
            "          28       0.37      0.50      0.43       100\n",
            "          29       0.34      0.25      0.29       100\n",
            "          30       0.24      0.56      0.34       100\n",
            "          31       0.26      0.22      0.24       100\n",
            "          32       0.31      0.22      0.26       100\n",
            "          33       0.37      0.34      0.35       100\n",
            "          34       0.18      0.23      0.20       100\n",
            "          35       0.20      0.21      0.20       100\n",
            "          36       0.30      0.19      0.23       100\n",
            "          37       0.20      0.25      0.22       100\n",
            "          38       0.13      0.06      0.08       100\n",
            "          39       0.28      0.37      0.32       100\n",
            "          40       0.20      0.17      0.18       100\n",
            "          41       0.73      0.46      0.56       100\n",
            "          42       0.24      0.14      0.18       100\n",
            "          43       0.27      0.25      0.26       100\n",
            "          44       0.14      0.09      0.11       100\n",
            "          45       0.10      0.08      0.09       100\n",
            "          46       0.19      0.24      0.21       100\n",
            "          47       0.47      0.49      0.48       100\n",
            "          48       0.38      0.70      0.49       100\n",
            "          49       0.41      0.42      0.42       100\n",
            "          50       0.16      0.07      0.10       100\n",
            "          51       0.14      0.14      0.14       100\n",
            "          52       0.61      0.48      0.54       100\n",
            "          53       0.34      0.53      0.41       100\n",
            "          54       0.47      0.47      0.47       100\n",
            "          55       0.00      0.00      0.00       100\n",
            "          56       0.46      0.45      0.46       100\n",
            "          57       0.36      0.36      0.36       100\n",
            "          58       0.30      0.42      0.35       100\n",
            "          59       0.17      0.27      0.21       100\n",
            "          60       0.71      0.70      0.70       100\n",
            "          61       0.41      0.52      0.46       100\n",
            "          62       0.26      0.37      0.30       100\n",
            "          63       0.37      0.27      0.31       100\n",
            "          64       0.13      0.10      0.11       100\n",
            "          65       0.15      0.25      0.19       100\n",
            "          66       0.16      0.06      0.09       100\n",
            "          67       0.37      0.22      0.28       100\n",
            "          68       0.69      0.67      0.68       100\n",
            "          69       0.65      0.28      0.39       100\n",
            "          70       0.60      0.12      0.20       100\n",
            "          71       0.55      0.36      0.44       100\n",
            "          72       0.12      0.17      0.14       100\n",
            "          73       0.35      0.20      0.25       100\n",
            "          74       0.23      0.23      0.23       100\n",
            "          75       0.45      0.67      0.54       100\n",
            "          76       0.29      0.70      0.41       100\n",
            "          77       0.14      0.17      0.15       100\n",
            "          78       0.20      0.08      0.11       100\n",
            "          79       0.23      0.36      0.28       100\n",
            "          80       0.12      0.03      0.05       100\n",
            "          81       0.22      0.26      0.24       100\n",
            "          82       0.68      0.53      0.60       100\n",
            "          83       0.21      0.16      0.18       100\n",
            "          84       0.20      0.05      0.08       100\n",
            "          85       0.33      0.49      0.40       100\n",
            "          86       0.26      0.49      0.34       100\n",
            "          87       0.32      0.29      0.30       100\n",
            "          88       0.12      0.26      0.16       100\n",
            "          89       0.23      0.36      0.28       100\n",
            "          90       0.23      0.27      0.25       100\n",
            "          91       0.49      0.37      0.42       100\n",
            "          92       0.26      0.17      0.20       100\n",
            "          93       0.24      0.09      0.13       100\n",
            "          94       0.71      0.64      0.67       100\n",
            "          95       0.51      0.27      0.35       100\n",
            "          96       0.24      0.16      0.19       100\n",
            "          97       0.14      0.40      0.21       100\n",
            "          98       0.16      0.13      0.15       100\n",
            "          99       0.25      0.24      0.24       100\n",
            "\n",
            "    accuracy                           0.30     10000\n",
            "   macro avg       0.31      0.30      0.29     10000\n",
            "weighted avg       0.31      0.30      0.29     10000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}